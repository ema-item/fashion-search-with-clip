{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 329006,
          "sourceType": "datasetVersion",
          "datasetId": 139630
        },
        {
          "sourceId": 12381743,
          "sourceType": "datasetVersion",
          "datasetId": 7807366
        },
        {
          "sourceId": 12399700,
          "sourceType": "datasetVersion",
          "datasetId": 7819454
        },
        {
          "sourceId": 12428673,
          "sourceType": "datasetVersion",
          "datasetId": 7839459
        },
        {
          "sourceId": 122721,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 103283,
          "modelId": 127514
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-10T20:47:07.308Z"
        },
        "id": "J4kQzKoLerz9",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": 72
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-10T20:47:07.308Z"
        },
        "id": "yPIPy3UUerz-",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": 73
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T11:22:43.777844Z",
          "iopub.execute_input": "2025-07-10T11:22:43.778073Z",
          "iopub.status.idle": "2025-07-10T11:22:55.160329Z",
          "shell.execute_reply.started": "2025-07-10T11:22:43.778045Z",
          "shell.execute_reply": "2025-07-10T11:22:55.159659Z"
        },
        "id": "d00MrKFoerz-",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": 74
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import clip\n",
        "import os"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T11:23:45.430715Z",
          "iopub.execute_input": "2025-07-10T11:23:45.431025Z",
          "iopub.status.idle": "2025-07-10T11:23:54.005527Z",
          "shell.execute_reply.started": "2025-07-10T11:23:45.430992Z",
          "shell.execute_reply": "2025-07-10T11:23:54.004976Z"
        },
        "id": "4ZlrQKTXer0A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clip"
      ],
      "metadata": {
        "id": "fpf_zDEngesE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device = device)\n",
        "\n",
        "def encode(image):\n",
        "    img = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feature = model.encode_image(img)\n",
        "    feature =feature.cpu().numpy()[0]\n",
        "\n",
        "    return feature"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-10T20:47:07.308Z"
        },
        "id": "excc0OHIer0A",
        "outputId": "476971f9-bc0c-487b-96e4-d3e617e0d2c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:07<00:00, 45.4MiB/s]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ArMUuQCk_dR",
        "outputId": "53306167-f9aa-4890-84b8-28526277cbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip Dataset"
      ],
      "metadata": {
        "id": "-sPnL8hvgjYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/clip_dataset/fashion_small_224.zip\" -d \"/content/fashion_small_224\""
      ],
      "metadata": {
        "id": "dy_CKKVcf9YQ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(\"/content/drive/MyDrive/clip_dataset/clip_features.npz\", allow_pickle=True)\n",
        "features = data['features'].astype('float32')\n",
        "paths = data['paths']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-10T20:47:07.309Z"
        },
        "id": "vVpWKhBDer0C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJH7ct3oBQ5h",
        "outputId": "7ddc578f-f91a-45fb-862a-0a575143eb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['30080.jpg' '9402.jpg' '57256.jpg' ... '7680.jpg' '12641.jpg' '3045.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(512)\n",
        "index.add(features)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-10T20:47:07.309Z"
        },
        "id": "HTUPgx2ver0E"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def buy(img):\n",
        "    return Image.fromarray(img)\n",
        "\n",
        "def show_image(image):\n",
        "\n",
        "    encode_user = encode(image)\n",
        "    encode_user = encode_user.reshape(1, -1).astype('float32')\n",
        "    distances, indices = index.search(encode_user, k=4)\n",
        "    similar_images = indices[0]\n",
        "    img1 = Image.open('/content/fashion_small_224/kaggle/working/fashion_small_224/'+ str(paths[similar_images[0]]))\n",
        "    img2 = Image.open('/content/fashion_small_224/kaggle/working/fashion_small_224/'+ str(paths[similar_images[1]]))\n",
        "    img3 = Image.open('/content/fashion_small_224/kaggle/working/fashion_small_224/'+ str(paths[similar_images[2]]))\n",
        "    img4 = Image.open('/content/fashion_small_224/kaggle/working/fashion_small_224/'+ str(paths[similar_images[3]]))\n",
        "\n",
        "    return np.array(img1), np.array(img2), np.array(img3), np.array(img4)\n",
        "\n",
        "with gr.Blocks(css=\".green-btn {background-color: #4CAF50 !important; color: white;}\") as demo:\n",
        "    gr.Markdown(\"Upload Image\")\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            image_input = gr.Image(height=800, label=\"Upload Image\", type=\"pil\")\n",
        "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "            clear_btn = gr.Button(\"Clear\", variant=\"stop\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    img1 = gr.Image(height=250, type=\"numpy\")\n",
        "                    btn_buy1 = gr.Button(\"BUY$\", elem_classes=\"green-btn\")\n",
        "                with gr.Column():\n",
        "                    img2 = gr.Image(height=250)\n",
        "                    btn_buy2 = gr.Button(\"BUY$\", elem_classes=\"green-btn\")\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    img3 = gr.Image(height=250)\n",
        "                    btn_buy3 = gr.Button(\"BUY$\", elem_classes=\"green-btn\")\n",
        "                with gr.Column():\n",
        "                    img4= gr.Image(height=250)\n",
        "                    btn_buy4 = gr.Button(\"BUY$\", elem_classes=\"green-btn\")\n",
        "\n",
        "\n",
        "\n",
        "    submit_btn.click(fn=show_image, inputs=image_input, outputs=[img1, img2, img3, img4])\n",
        "    clear_btn.click(fn=lambda: [None, None, None, None, None], inputs=[], outputs=[image_input, img1, img2, img3, img4])\n",
        "\n",
        "    btn_buy1.click(fn=buy, inputs=img1, outputs=image_input)\n",
        "    btn_buy2.click(fn=buy, inputs=img2, outputs=image_input)\n",
        "    btn_buy3.click(fn=buy, inputs=img3, outputs=image_input)\n",
        "    btn_buy4.click(fn=buy, inputs=img4, outputs=image_input)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-10T11:33:56.181799Z",
          "iopub.execute_input": "2025-07-10T11:33:56.18257Z",
          "iopub.status.idle": "2025-07-10T11:33:56.738123Z",
          "shell.execute_reply.started": "2025-07-10T11:33:56.182544Z",
          "shell.execute_reply": "2025-07-10T11:33:56.737595Z"
        },
        "id": "gJguhsGher0F",
        "outputId": "e6da5300-cc44-4811-b6a6-c9a347dced11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://feac83b63a3765eb79.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://feac83b63a3765eb79.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "execution_count": 71
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------\n",
        "\n",
        "------------\n",
        "\n",
        "-----------"
      ],
      "metadata": {
        "id": "hrO8_ZRTThMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing in Kaggle with big Dataset with 44,441 image"
      ],
      "metadata": {
        "id": "7rkphP_ogs4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_path = '/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/'\n",
        "\n",
        "# images_feature_list = []\n",
        "# images_path_list = []\n",
        "\n",
        "# for image in os.listdir(dataset_path):\n",
        "#     image_path = os.path.join(dataset_path, image)\n",
        "#     img = Image.open(image_path)\n",
        "\n",
        "#     feature_image = encode(img)\n",
        "#     images_feature_list.append(feature_image)\n",
        "#     images_path_list.append(image_path)\n",
        "\n",
        "# np.savez(\"clip_features.npz\", features=images_feature_list, paths=images_path_list)"
      ],
      "metadata": {
        "id": "XVC96hQ8eTRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting 10,000 images at random to continue the project in Google Cloud and managing resource limits and cloud limits for Gradio"
      ],
      "metadata": {
        "id": "qDJAkMCThl1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_path = '/kaggle/input/fashion-product-images-dataset/fashion-dataset/images/'\n",
        "# output_dir = \"/kaggle/working/fashion_small_224/\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# N = 10000\n",
        "\n",
        "# images_feature_list = []\n",
        "# images_path_list = []\n",
        "\n",
        "# all_images = [f for f in os.listdir(dataset_path) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "# print(f\"total number of images: {len(all_images)}\")\n",
        "\n",
        "# selected_images = random.sample(all_images, min(N, len(all_images)))\n",
        "\n",
        "# for img_name in tqdm(selected_images, desc=\"compressing...\"):\n",
        "\n",
        "#     img_path = os.path.join(dataset_path, img_name)\n",
        "#     img = Image.open(img_path)\n",
        "#     img = img.resize((224, 224))\n",
        "\n",
        "#     feature_image = processing(img)\n",
        "#     images_feature_list.append(feature_image)\n",
        "\n",
        "#     save_path = os.path.join(output_dir, os.path.splitext(img_name)[0] + \".jpg\")\n",
        "#     img.save(save_path, format=\"JPEG\", quality=85, optimize=True)\n",
        "\n",
        "#     images_path_list.append(img_name)\n",
        "\n",
        "\n",
        "# print(len(images_feature_list))\n",
        "# print(len(images_path_list))\n",
        "\n",
        "# np.savez(\"clip_features.npz\", features=images_feature_list, paths=images_path_list)"
      ],
      "metadata": {
        "id": "wOStoJSldeOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}